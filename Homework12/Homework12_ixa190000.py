# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FVeSjJ-w8B8bGU1CWTkO3GKjljVZCPiv

### Text Classification 2

#### Ikhlaq Ahmad
#### ixa190000
#### Dr. Karen Mazidi
#### CS 4395
#### Text Classification 2

## Overview
#### This program is a basic implementation of classifying text data using Recurrent Neural Networks (RNN) and Convolution Neural Networks (CNN).

## Model
#### The model has been trained using Rotten Tomatoes movies' review data. The model uses 80% of the data for training and 20% for making a prediction based on that model.
"""

# Dependencies
import pandas as pd
import numpy as np
import tensorflow as tf

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense

# Import the csv file using files upload
from google.colab import files
uploaded = files.upload()

# Put the acutal file name in the function
file_data = pd.read_csv("data_rt.csv", header=0)

# Prints rows and columns in the file
print('rows and columns:', file_data.shape)

# Prints the whole file from the beginning
print(file_data.head())

# Print reviews and labels columns
print(file_data["reviews"])
print(file_data["labels"])

# Tokenize the reviews using maximum of 5000 reviews
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(file_data["reviews"])
X = tokenizer.texts_to_sequences(file_data["reviews"])

# pad the data to a maximum of 500 words per review
max_words = 500

# Pad sequences
X = pad_sequences(X, maxlen=max_words)

# One-hot encode labels
y = np.eye(2)[file_data["labels"]]

# Split the data into training and testing sets
split_ratio = 0.8
split_index = int(split_ratio * len(X))

# Assign X and y values with training and testing data

# X values
X_train = X[:split_index]
X_test = X[split_index:]

# y values
y_train = y[:split_index]
y_test = y[split_index:]

# Convert test labels to one-hot encoding
y_test_labels = np.argmax(y_test, axis=1)

# Define RNN model
rnn_model = Sequential()
rnn_model.add(Embedding(5000, 32, input_length=max_words))
rnn_model.add(LSTM(100))
rnn_model.add(Dense(2, activation='softmax'))
rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train RNN model and save it in history using epochs = 10, and batch_size = 64
history = rnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)

# Prints the model summary after training
print(rnn_model.summary())

# Graph plot dependencies
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

# Make predictions using the RNN model
y_pred_rnn = rnn_model.predict(X_test)
y_pred_rnn = np.argmax(y_pred_rnn, axis=1)

# Generate classification report for RNN model
print("Classification Report for RNN Model:")
print(classification_report(y_test_labels, y_pred_rnn))

# Generate accuracy and loss plots for RNN model for accuracy vs loss
plt.plot(history.history["accuracy"], label='Training accuracy')
plt.plot(history.history['val_accuracy'], label='Validation accuracy')
plt.title('RNN model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history["loss"], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('RNN model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Generate confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns


cm = confusion_matrix(y_test_labels, y_pred_rnn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion matrix for RNN model')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

"""## Same model using CNN approach

"""

# Define CNN model
cnn_model = Sequential()
cnn_model.add(Embedding(5000, 32, input_length=max_words))
cnn_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(LSTM(100))
cnn_model.add(Dense(2, activation='softmax'))

# Compile the CNN Model
cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train CNN model using epochs = 10, and batch_size = 64
history = cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)

# Summary of CNN Model
print(cnn_model.summary())

# Make predictions CNN model using the testing data

# Make predictions using the RNN model
y_pred_cnn = cnn_model.predict(X_test)
y_pred_cnn = np.argmax(y_pred_cnn, axis=1)

# Generate classification report for CNN model
print("Classification Report for CNN Model:")
print(classification_report(y_test_labels, y_pred_cnn))

# Generate accuracy and loss plots for CNN model for accuracy vs loss 
plt.plot(history.history["accuracy"], label='Training accuracy')
plt.plot(history.history['val_accuracy'], label='Validation accuracy')
plt.title('CNN model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history["loss"], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('CNN model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Make predictions using the CNN model
y_pred_cnn = cnn_model.predict(X_test)

# Convert predicted probabilities to class labels
y_pred_cnn = np.argmax(y_pred_cnn, axis=1)

# Generate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred_cnn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion matrix for CNN model')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

"""## Analysis
The RNN and CNN models were both trained on the same dataset of Rotten movie reviews and achieved comparable accuracy scores. The RNN model achieved a final accuracy of **0.65** on the test set, while the CNN model achieved a slightly higher accuracy of **0.48**.

Both models were able to learn the semantic meaning of words and identify patterns in the data to classify positive and negative movie reviews with an average accuracy. However, the RNN model outperformed the CNN model in terms of training time, achieving the same level of accuracy in fewer epochs.

Overall, both models demonstrate the effectiveness of deep learning techniques in natural language processing tasks such as sentiment analysis. The choice of model to use will depend on the specific task requirements and available computing resources. In cases where fast training times are important, a CNN model may be preferred over an RNN model. Conversely, if the data has a sequential structure or temporal dependencies, an RNN model may be more appropriate.
"""